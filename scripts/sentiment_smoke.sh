#!/bin/bash
# scripts/sentiment_smoke.sh
# PR-F.4 Social Sentiment Stub & Data Enrichment - Smoke Test

set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ROOT_DIR="$(cd "$(dirname "${SCRIPT_DIR}")" && pwd)"

echo "[sentiment_smoke] Starting sentiment smoke test..." >&2

# Python test script
python3 << PYTHON_TEST
import sys
import json
import os

# Add root to path
sys.path.insert(0, '$ROOT_DIR')

from ingestion.sentiment import (
    SentimentEngine,
    StubSentimentEngine,
    create_sentiment_engine,
)

# Test counters
passed = 0
failed = 0

def test_case(name, condition, msg=""):
    global passed, failed
    if condition:
        print(f"  [sentiment] {name}: PASS", file=sys.stderr)
        passed += 1
    else:
        print(f"  [sentiment] {name}: FAIL {msg}", file=sys.stderr)
        failed += 1

print("[sentiment_smoke] Testing sentiment engine interface...", file=sys.stderr)

# Test 1: StubSentimentEngine interface
engine = StubSentimentEngine()
test_case("stub_engine_created", engine is not None)

# Test 2: MINT_PUMP returns ["pump"]
tags = engine.analyze("MINT_PUMP")
test_case("mint_pump_returns_pump", tags == ["pump"], f"got {tags}")

# Test 3: MINT_SCAM returns ["scam"]
tags = engine.analyze("MINT_SCAM")
test_case("mint_scam_returns_scam", tags == ["scam"], f"got {tags}")

# Test 4: MINT_BUZZ returns ["buzz", "social"]
tags = engine.analyze("MINT_BUZZ")
test_case("mint_buzz_returns_multiple", tags == ["buzz", "social"], f"got {tags}")

# Test 5: Unknown mint returns empty
tags = engine.analyze("UNKNOWN_TOKEN")
test_case("unknown_returns_empty", tags == [], f"got {tags}")

# Test 6: Custom tags override
custom_engine = StubSentimentEngine(custom_tags={
    "CUSTOM": ["custom_tag", "test"]
})
tags = custom_engine.analyze("CUSTOM")
test_case("custom_tags_override", tags == ["custom_tag", "test"], f"got {tags}")

# Test 7: Case-insensitive matching
tags = engine.analyze("mint_pump")  # lowercase
test_case("case_insensitive", "pump" in tags, f"got {tags}")

# Test 8: Factory function with disabled config
disabled_config = {"sentiment": {"enabled": False}}
disabled_engine = create_sentiment_engine(disabled_config)
tags = disabled_engine.analyze("MINT_PUMP")
test_case("disabled_returns_empty", tags == [], f"got {tags}")

# Test 9: Factory function with stub provider
stub_config = {
    "sentiment": {
        "enabled": True,
        "provider": "stub",
        "stub_tags": {"TEST_MINT": ["test_tag"]}
    }
}
stub_engine = create_sentiment_engine(stub_config)
tags = stub_engine.analyze("TEST_MINT")
test_case("factory_stub_enabled", tags == ["test_tag"], f"got {tags}")

# Test 10: Test with paper_pipeline.py imports
print("[sentiment_smoke] Testing pipeline integration...", file=sys.stderr)

try:
    from integration.paper_pipeline import run_paper_pipeline
    from integration.config_loader import load_config

    # Load sentiment config
    config_path = "$ROOT_DIR/integration/fixtures/config/sentiment.yaml"

    # Verify the config is loadable
    config = load_config(config_path)
    test_case("sentiment_config_loadable", config is not None)

    # Check config has sentiment enabled
    sentiment_cfg = config.get("sentiment", {})
    test_case("sentiment_enabled", sentiment_cfg.get("enabled") == True)
    test_case("sentiment_provider_stub", sentiment_cfg.get("provider") == "stub")

except ImportError as e:
    print(f"  [sentiment] pipeline_import: SKIP (integration not available)", file=sys.stderr)
    passed += 1  # Skip gracefully
except Exception as e:
    print(f"  [sentiment] pipeline_test: FAIL {e}", file=sys.stderr)
    failed += 1

# Test 11: Simulate pipeline behavior with sentiment enrichment
print("[sentiment_smoke] Testing sentiment enrichment simulation...", file=sys.stderr)

# Simulate what paper_pipeline does with sentiment
def simulate_sentiment_enrichment(trade_mint: str, sentiment_engine: SentimentEngine) -> dict:
    """Simulate the sentiment enrichment step from paper_pipeline."""
    try:
        tags = sentiment_engine.analyze(trade_mint)
        return {"sentiment_tags": tags}
    except Exception as e:
        # Fail-safe: exceptions caught, return empty tags
        print(f"[sentiment] WARNING: sentiment analysis failed: {e}", file=sys.stderr)
        return {"sentiment_tags": []}

# Create engine
engine = create_sentiment_engine({"sentiment": {"enabled": True, "provider": "stub"}})

# Test with pump mint
result = simulate_sentiment_enrichment("MINT_PUMP", engine)
test_case("enrichment_pump", result["sentiment_tags"] == ["pump"], f"got {result}")

# Test with scam mint
result = simulate_sentiment_enrichment("MINT_SCAM", engine)
test_case("enrichment_scam", result["sentiment_tags"] == ["scam"], f"got {result}")

# Test with unknown mint
result = simulate_sentiment_enrichment("RANDOM_MINT", engine)
test_case("enrichment_unknown", result["sentiment_tags"] == [], f"got {result}")

# Test 12: Verify _build_signal_row would include sentiment_tags
print("[sentiment_smoke] Testing signal row integration...", file=sys.stderr)

def build_signal_row_with_sentiment(
    mint: str,
    sentiment_tags: list,
) -> dict:
    """Simulate building a signal row with sentiment_tags (like paper_pipeline does)."""
    row = {
        "schema_version": "signals.v1",
        "mint": mint,
        "decision": "entry_ok",
    }
    # Add sentiment_tags to the row (as paper_pipeline does)
    row["sentiment_tags"] = sentiment_tags
    return row

row = build_signal_row_with_sentiment("MINT_PUMP", ["pump"])
test_case("signal_row_has_sentiment_tags", "sentiment_tags" in row)
test_case("signal_row_sentiment_value", row["sentiment_tags"] == ["pump"])

# Summary
print(f"\n[sentiment_smoke] Tests: {passed} passed, {failed} failed", file=sys.stderr)

if failed > 0:
    sys.exit(1)
else:
    print("[sentiment_smoke] OK âœ…", file=sys.stderr)
    sys.exit(0)
PYTHON_TEST

echo "[sentiment_smoke] Smoke test completed." >&2
